# Explainium Configuration
# Copy this file to `.env` and adjust settings as needed.

# Environment
EXPLAINIUM_ENV=development

# GPU Configuration
ENABLE_GPU=true
# Options: auto, metal, cuda, cpu
GPU_BACKEND=auto
# Set to a positive number; -1 uses all layers
LLM_GPU_LAYERS=-1
# Fraction of available GPU memory to use 
GPU_MEMORY_FRACTION=0.8

# File Processing
MAX_FILE_SIZE_MB=50
PROCESSING_TIMEOUT=300
CHUNK_SIZE=2000  # doubles as the per-chunk character cap if CHUNK_MAX_CHARS is not set

# API Settings
API_HOST=127.0.0.1

# LLM Defaults
LLM_N_CTX=8192
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=1536

# Quality & Performance
CONFIDENCE_THRESHOLD=0.8
QUALITY_THRESHOLD=0.7
MAX_WORKERS=8

LLM_PARALLEL_INSTANCES=2

# Chunking
# Options: fixed, breakpoint_semantic, dsc
CHUNKING_METHOD=fixed
CHUNK_MAX_CHARS=2000  # optional override for the chunk character cap
EMBEDDING_MODEL_PATH=models/embeddings/all-mpnet-base-v2
SEM_SIMILARITY=cosine
SEM_MIN_SENTENCES_PER_CHUNK=2
SEM_MAX_SENTENCES_PER_CHUNK=40
SEM_LAMBDA=0.15
SEM_WINDOW_W=30
DSC_PARENT_MIN_SENTENCES=10
DSC_PARENT_MAX_SENTENCES=120
DSC_DELTA_WINDOW=25
DSC_THRESHOLD_K=1.0
DSC_USE_HEADINGS=true
DEBUG_CHUNKING=false
