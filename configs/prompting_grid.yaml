# Configuration for prompting experiments

out_root: logs/prompting_grid

# LLM settings - Match working baseline configuration
llm:
  backend: "llama_cpp"
  temperature: 0.25
  max_tokens: 1024
  num_workers: 1
  device_strategy: "single"

# Documents to process
documents:
  - id: "3M_OEM_SOP"
    path: "datasets/archive/test_data/text/3m_marine_oem_sop.txt"
    gold: "datasets/archive/gold_human/3M_OEM_SOP.json"
  - id: "DOA_Food_Proc"
    path: "datasets/archive/test_data/text/DOA_Food_Man_Proc_Stor.txt"
    gold: "datasets/archive/gold_human/DOA_Food_Man_Proc_Stor.json"
  - id: "op_firesafety_guideline"
    path: "datasets/archive/test_data/text/op_firesafety_guideline.txt"
    gold: "datasets/archive/gold_human/op_firesafety_guideline.json"

# Chunking settings (a default strategy for all experiments)
chunking:
  method: "dsc"

# Prompting experiments to run
prompting_experiments:
  - name: "P0_zero_shot"
    strategy: "P0"
  - name: "P1_few_shot"
    strategy: "P1"
  - name: "P2_cot"
    strategy: "P2"
  - name: "P3_two_stage"
    strategy: "P3"

# Evaluation settings
evaluation:
  tier: "A"
  threshold: 0.75
