Problem Definition & Baseline Evaluation
Task formalization
Let d∈D denote an industrial document (e.g., SOP, guideline, manual). Our task is to extract a machine-actionable representation of the procedures in d that document that preserves (1) the set of steps, (2) their execution order, and (3) any procedural constraints (guards/conditions and resource requirements).
Formally, we input raw text, provided by a format-aware processing pipeline that extracts text from multiple modalities (e.g., text/PDF/Word/HTML/CSV/PPTX; OCR for images/scans; ASR for audio). In the baseline, d is processed in non-overlapping fixed-sized chunks. Furthermore, an instruction-tuned LLM processes each chunk independently and produces local chunk-level extractions that are merged into a single prediction for d.
As a presentation of the baseline extractions target, a document’s procedural content is modelled by :
	A finite set of steps S={s_1,…,S_n } with a (partial or total) execution order π:S→{1,…,n};
	And a set of constraints C={c_1,…,c_m } (e.g., guards, safety, pre/post-conditions, parameter or resource requirements), with an attachment relation attach:C→2^S linking each constraint to the step(s) it constrains.
From π we derive adjacency A={(s_i,s_j )∣π(s_j )=π(s_i )+1}.
The system outputs primary baseline tier-A extractions, returned as a flat JSON object with :
	steps=[{id,text,order},…] where order encodes π;
	optional constraints=[{id,text,…},…] as free-text statements, ideally referencing step IDs, attachments are included only if the LLM produced explicit step references;
	optional entities (facts/parameters/tools) used for analysis but not scored in the baseline headline metric.
These results are evaluated on four targets (Tier-A):
	Step identification. Soft semantic match between our predictions and the manually noted gold steps, scored with Precision/Recall/F1 (Step F1). Matching uses SBERT embeddings with Hungarian 1-to-1 alignment and a similarity threshold τ_s set by default to τ_s=0.75
	Local order. Adjacency F1 over the prediction vs gold bigrams (s_i,s_(i+1) ).
	Rank coherence. Kendall’s τ on the aligned step ranks, normalised to [0,1] via (τ+1)/2
	Constraints:
	Coverage: fraction of gold constraint statements that are found
	Attachment F1: correctness of the “constraint to step” linking where links are present.
We report those four targets into a compact headline score for T.ier-A extractions of : 
A_score=0.7⋅StepF1+0.3⋅AdjacencyF1
For completeness, we also derive a graph view out of the JSON object predictions and a gold for a second Tier-B evaluation, steps become nodes(type=” step”); constraints become nodes(type=”condition”); next edges are emitted between consecutive steps, condition_on edges are emitted if explicit references exist. We report GraphF1 and edge-type diagnostics (NEXT_EdgeF1; Logic_EdgeF1) as lower bounds; they are not headline results until native graph emission is implemented (see chapter 4).
Dataset & documents
We experimented on three industrial documents with manually curated gold (step lists with IDs and constraint phrases; constraint->step links when definitive). These documents are as follows:
Table 1. Document title, description and preview of our 3 dataset files (own table)
3M_OEM_SOP	A 3M Marine Standard Operating Procedure handbook covering surface preparation and application procedures (stepwise instructions)	 
DOA_Food_Proc	A pest-control training manual for food manufacturing/processing/storage (regulatory + procedural content)	 
op_firesafety_guideline	Fire-safety guidelines for working machines (risks, prevention, operating procedures)	 

More about the preprocessing? Reasons for choosing these exact 3? Sources may be too?
Baselines (flat extractor + naive ordering; any public systems if used)
The baseline is a local running, instruction-prompted extractor built on llama.cpp with Mistral-7B-Instruct (GGUF). This open-weight model strikes a balance between strong reasoning and efficiency, making it ideal for understanding procedural text. Its compact 7B size allows us to run local reproducible experiments. Decoding uses low-temperature settings :
Table 2. Mistral-7B-Instruct baseline extraction: Model decoding and filtering parameters (Own table)
Setting	Value	Description 
temperature	0.1	Low value chosen to minimize creativity and maximize stability; ensures deterministic and factual JSON generation.
top_p	0.9	Nucleus sampling that considers the top 90% cumulative probability mass, balancing fluency with robustness against rare, erratic completions.
max_tokens	1536	Maximum number of tokens generated per response; prevents runaway generations while allowing detailed structured outputs.
repeat_penalty	1.1	Penalizes repeated n-grams to reduce loops and redundant text in long generations.
n_ctx	8192	Model context window size; supports processing long prompts and large document chunks without truncation.
chunk_size	2000 (characters)	Text segmentation size per LLM call; fits within context limits and reduces semantic bleed between chunks.
llm_max_chunks	0 (unlimited)	No upper limit on processed chunks, ensuring complete document coverage without silent truncation.
confidence_threshold	0.8	Minimum per-item confidence, filters out weak or uncertain extractions.
quality_threshold	0.7	Document-level aggregate confidence threshold, flags low-quality or noisy extractions for manual review.

The prompt we used is short and clean:
“[INST] You produce lightweight procedural structure from {document_type} documents… Return a SINGLE JSON object with: steps (S1, S2, …), constraints (C1, …, referencing steps), and entities (supporting facts). Return only the JSON object… [/INST]” 
Inspired by Xu et al. (2024)’s description of generative IE prompts, which combine concise task framing, explicit output schema definition, and constrained decoding for structural consistency, the prompt is designed to extract procedural knowledge with minimal linguistic noise. 
The use of explicit instruction tags ([INST] … [/INST]) delineates model scope, improving reproducibility and ensuring deterministic JSON output suitable for automated parsing. The schema explicitly separates procedural steps, conditional constraints, and factual entities, allowing the model to represent order and dependencies in a machine-readable yet interpretable form. This setup produces a flat list of steps (implicitly ordered by list position), accompanied by optional free-text constraints and contextual entities supporting each step.

Minimal schema/ontology (nodes, edges, constraints; one worked toy example) 
To support a better, improved structured extraction, we define a minimal procedural schema, where names are lowercase at the JSON level :
Node (or vertex) types : 
	step executable instructions; fields: {id, text, order?, context?}
	condition guard/requirement/safety note; fields: {id, expression, type?, context?}
	equipment tool/material/asset; fields: {id, name, model?, category?, vendor?, notes?}
	parameter named parameter/threshold, fields: {id, name, value?, unit?, range?, notes?}
(Fields followed by a question mark indicate they are optional (e.g., value?))
Edge (or link) types :
	next temporal adjacency between steps(primary) 
	condition_on a condition attached to a step it constrains (primary)
	Reserved: uses, has_parameter, requires, produces, references, alternative_to
Example :
 
Figure 3. Example of a lightweight procedural knowledge graph in JSON format (own illustration).
Proposed Methods 
Document preprocessing & semantic chunking
Starting with the baseline implementation, we can see how it lacks in many aspects. Initially, from the very first point, chunking —the splitting that the documents undergo —to send individual chunks (groups of text) from the full text to the LLM to process one chunk at a time. The issue lies in the chunking method itself; the current pipeline splits the document into fixed-size text chunks, defaulting to 2000 characters, or approximately 500 tokens (1 token equals 4 characters). Even though this is good in how it improves the throughput, it still raises a big problem:
It breaks cross-chunk continuity; sentences, lists, or even words can be cut mid-way, and the context established in one chunk is unavailable for the next one in line, since they are being analysed individually, separated, and presented one after the other. 
For example, in the document “op_firesafety_guideline.pdf” two consecutive chunks (the fourth and fifth ones) show a sentence split across the boundary, even on the word level, “maintenance” was split into “.. mai” and “ntenance …” as the table shows : 
... instructions. During maintenance, the machine is ... 

Table 3. Comparison of two consecutive chunk previews (own table).
4th chunk : (preview of first 200 chars)	5th chunk : (preview of first 200 chars)
does the risk of fire impact the company’s other 
risks?
• Is the company taking on a suitable risk?
• What action will the company’s employees take in the 
event of an accident?
• Is preventive s...	ntenance according to the manu­
facturer’s instructions. During maintenance, the machine 
is also inspected for certain risks of fire, mostly related to 
potential leaks.
If the company has its own se...
Similarly, the 4th chunk starts with a question “does the risk of…”, the last of many questions processed back in the 3rd chunk. 
This fragmentation leads to a loss of meaning and logic in the chunks, as well as the relationships between entities across different chunks, which leads us to question how we can effectively chunk large documents more efficiently, without dramatically increasing the chunk size or losing meaning across chunk borders.
Breakpoint-based semantic chunking
An instant, simple way to preserve more context is to use chunk overlap in our text splitting. This is achieved by overlapping a few tokens/characters/sentences between neighbouring chunks, both of which contain the shared context. For example, if Chunk#1 ends with a sentence and Chunk#2 starts mid-idea, both chunks can include the shared sentence, so whichever chunk is returned, the relevant idea is not lost. 
Example:
We could set an input to be five bullet points about an industrial/procedural text:
1. Turn off the main valve before starting any maintenance.
2. Remove the protective cover from the assembly.
3. Inspect the gaskets for signs of wear or leakage.
4. Replace gaskets if any damage is found.
5. Reinstall the protective cover securely.
Let us use two sentences for a chunk size, paired with a one-sentence chunk overlap (so the last sentence of the previous chunk also appears at the start of the next one). We then get the following results : 
1. Turn off the main valve before starting any maintenance.
2. Remove the protective cover from the assembly.
3. Inspect the gaskets for signs of wear or leakage.
4. Replace gaskets if any damage is found.
5. Reinstall the protective cover securely.
Compared to, with the overlap (1 sentence):
1. Turn off the main valve before starting any maintenance.
2. Remove the protective cover from the assembly.
3. Inspect the gaskets for signs of wear or leakage.
4. Replace gaskets if any damage is found.
5. Reinstall the protective cover securely.
The dark blue highlighted zones are in common with both the previous and next chunks; this is especially beneficial in the context of industrial or procedural text, where each step depends heavily on the previous step’s context. In this example, for instance, our system gets both “Remove the protective cover from the assembly.” AND “Inspect the gaskets for signs of wear or leakage,” so we know what step we are on and what to do next.
While chunk overlap helps prevent context loss at chunk boundaries, it still cannot capture deeper semantic relationships or dependencies between non-adjacent sentences, because, after all, it is still a simple static character/sentence splitting method. This leads us to different techniques, such as embedding-based clustering methods, which are preferable for grouping truly related content based on meaning, rather than just proximity.
Conventional chunkers apply uniform window sizes regardless of the semantic content. We find it quite bizarre how we can set a single global constant value for chunk size without ever considering the actual content itself. 
Embeddings represent the semantic meaning of a string. They do not do much on their own, but when compared to embeddings of other texts, you can start to infer the relationship between chunks. We aim to leverage this property and explore the use of embeddings to identify clusters of semantically similar texts. More specifically, our study will focus on finding breakpoints between sequential sentences. The system starts at sentence #1, gets the embedding, and compares it to the next one in line, sentence #2, then to sentence #3 and so on. It is looking for “break points” where the embedding distance was significant. If it exceeds a threshold, it is considered the start of a new semantic section. This approach, popularised by Kamradt and subsequently integrated into frameworks such as LangChain and LlamaIndex, has gained traction in retrieval-augmented generation systems for its ability to preserve semantic coherence within chunks. 
We can now formalise this intuition as an optimisation problem, following the framework established by Alemi and Ginsparg for text segmentation using semantic embeddings. Let us consider a document of n sentences and let e_1,e_2,…,e_n denote the embedding vectors for these sentences, where each individual e_i∈R^d represents the semantic content of the sentence i in an d-dimensional embedding space. (Alemi & Ginsparg, 2015).
For all consecutive elements e_t spanning from i to j-1, we can start by defining a cohesion score:
c(i,j)=1/(j-i) ∑_(t=i)^(j-1)▒sim(e_t,e_(t+1) ) 
Where sim(e_t,e_(t+1) ) measures the semantic similarity between the two consecutive elements at positions t and t+1, in our case, simple cosine similarity, the summation calculates the total similarity for all pairs in this interval, and dividing it by j-i gives the mean similarity value across the segment. As a result, this score quantifies the average semantic coherence within a segment, where a high value indicates that the consecutive sentences are semantically close or related, and lower values suggest a topic shift, recommending the start of a new chunk. 
Our goal thus becomes clear: splitting the sequence [1,n] into a set of smaller segments S={[i_1,j_1 ],[i_2,j_2 ],…,[i_k,j_k ]} that maximise our objective function:
J(S)=∑_([i,j]∈,S)▒〖c(i,j)-λ∣S∣〗
With ∣S∣ denoting the number of segments (chunks), and λ>0 as a break penalty, the idea is to reward high internal coherence/cohesion within each segment (c(i,j)), and penalise excessive fragmentation by adding a cost to each additional break point of segments λ∣S∣. To our knowledge, this has not been published in precisely this form in the text segmentation literature; however, it builds upon the prior work mentioned above and is inspired by the segmentation literature (e.g., Martin et al., 2010; embedding-based segmentation; etc.). The parameter λ thus holds a significant role and importance in controlling the granularity of the segmentation; small values allow higher break frequencies at semantic boundaries, while larger values result in fewer, but longer chunks.
From a programming perspective, this objective can be computed efficiently using dynamic programming. We adapt this by first defining DP[j] the maximum objective value achievable for segmenting the prefix [1,j]. Our recurrence relation would be :
DP[j]=〖max〗_(i<j) {DP[i]+c(i,j)-λ}
With the base case DP=0. This recursion considers all possible positions i<j for the final segment boundary, selecting the one that maximises the cumulative score.
The time complexity can be further lowered from O(n^2) to O(nw) by restricting the search to a window of size w, since logically semantically coherent segments usually do not span long distances apart.

Now I show code maybe?
Dual Semantic Chunker (DSC)
In their very recent work, Toresan, Moreira, Paula, and Bencke (2025) demonstrated that despite semantic chunking being highly effective, and preserving meaning a lot better than traditional fixed or syntactic chunking methods, it still can, in lots of cases, over-segment or under-segment when the similarity signal is noisy or too local, this is due to the heavy reliance of semantic chunkers on embedding-based similarity between sentences or blocks, and failing by :
Losing global coherence, in some cases, local embedding similarity may fail to capture long-range dependencies that span across extended contexts, which can compromise the model's quality and long-term performance on long documents, particularly when similar contexts persist for an extended period, causing different boundary decisions, potentially causing the embedding model to drift, making the method unstable across domains or document types. Another issue is how these embedding models may generate inconsistent chunk sizes. Embeddings will vary by topic density, which poses the possibility of producing either very short or very long chunks, making it challenging to balance quality and quantity. Furthermore, our last downside to adopting semantic chunking is that a single semantic pass cannot distinguish between micro-semantic continuity (e.g., sentence level) and macro-semantic structure (e.g., section level), leading to blurred or redundant segments that overlook the hierarchical text structure.
These weaknesses are what motivate the Dual Semantic Chunker (DSC).
From its name, DSC introduces a two-stage approach :
	First, our input text is split into blocks or parental chunks (Large context units)
	Then, the semantic representations computed for the blocks or parental chunks are leveraged to decide on optimal splitting points, giving birth to smaller, more precise, granular semantic units, called child chunks (Toresan, Moreira, Paula, and Bencke 2025)
The process is visualised below in Figure 4. :
 
Figure 4. Overview of the Dual Semantic Chunker (Self produced).
Though our most significant issue was adjacency F1 and constraint to step attachment, so maybe we can keep making this just an extra method and not a main one? 
Sentence-level blocks => SBERT embeddings => similarity matrix with local diagonal band; + a new dynamic threshold θ = μ + k·σ => POTENTIALLY EQUALS strong retrieval (DCG/F1/P@5) over Fixed/Recursive/semantic based 
And after that, we will show: Fixed vs. Semantic-A vs. DSC => effect sizes on our macro metrics.

Prompting & schema-constrained JSON/TOON emission
Prompting baseline
Having addressed how the documents are segmented into semantically stable chunks, with good information retention, the following methodological challenge lies in how the model is instructed. The limitation is no longer what the model reads, but rather how the model is instructed to read. This makes prompt design our core mechanism for steering the LLM behaviour, to enforce structure and reduce variability in our procedural knowledge extraction.
By now, with either Dual Semantic Chunking (DSC) implemented or the more straightforward Embedding-Based Breakpoint Chunking (EBBC), our LLM’s calls should now receive coherent, self-contained textual segments that capture a locally consistent part of the text. EXPLAIN ZERO SHOT AND ALLAT Currently, our baseline system uses a single-shot, instruction-following prompt that : 
	Gives the model a role (“you produce lightweight procedural structure”) 
	Specifies a strict JSON schema (steps/constraints/entities)
	Enforces ID rules (S1, S2, … ; C1, C2, …)
	Includes guidelines (steps in execution order, constraints as requirements, concise phrasing)
	Forces JSON-only output,
	It is used in parallel per chunk, then merged (meaning independent simultaneous chunk processing and extraction, and only merging all the results after all chunks complete)
To better illustrate this workflow, we can take this example to showcase what the model receives as input. Let us say we were dealing with a fire safety document, and one chunk contains the following 2000 chars : 
“Employees must inspect fire extinguishers monthly, checking the pressure gauge to ensure it is within the green zone of 12 to 18 bar (174 to 261 psi). If the gauge shows low pressure, replace or recharge the extinguisher immediately. All personnel should be trained on the proper use of extinguishers before operating them. The PASS technique should be followed: Pull the pin, Aim at the base of the fire, Squeeze the handle, and Sweep side to side. Extinguishers must be mounted at accessible heights, with no more than 5 feet between the floor and the top of the extinguisher.”
The exact prompt sent to the LLM, in our baseline system, would be :
 
The model responds with this output under JSON format:
 
Although the model made seven step extractions ("steps_extracted": 7) and identified one specific spec entity ("entities_extracted": 1), it still failed to extract any constraints, despite the text clearly containing many constraints and requirements (must, should, if…then, height limits), like: 
“Employees must … monthly”
“If the gauge shows low pressure, replace or recharge the extinguisher immediately”
“All personnel should be trained…” etc…
So what went wrong exactly? The model received a well self-contained snippet, yet the extraction was still not ideal, so it can not be a chunking issue. We could present the entire paragraph as one chunk, or the entire document – the result would be the same.
This indicates how our current prompt under-specifies the notion of ‘constraint’, and the model instead overgeneralizes them as more procedural steps. Since we used zero-shot without examples, the model never had a demonstration of what a constraint looks like compared to a step. This aside, another factor could be how the model does not reason in the process of deciding what is what; no Chain-of-Thoughts (CoT) means our model never explicitly thinks “this is a rule that modifies a step”, for example.
Another possible, simpler reason is that the definition of constraints in our JSON schema might just be too vague or buried! “constraints capture requirements and prerequisites” is too abstract.
This drives us to revisit not only the schema, but the entire prompting strategy.
Carriero et al. (2024) state, "In the broader LLM literature, three families of prompting have emerged as particularly relevant to our setting: zero-shot instruction prompting, few-shot in-context learning, and chain-of-thought prompting" (para. X). Each of these approaches provides a distinct mechanism for what the model is instructed to do. Zero-shot prompting, by definition, is the most straightforward method for instructing a large language model (LLM); it exposes the model solely to natural language instructions and possibly a task description too, but without any task-specific examples, such as: “I need you to ... this and this and that.”. This is not just what most typical LLM users use in their prompts, but what our baseline system does as well: It uses a role description (“You produce lightweight procedural structure from manual documents”) plus a JSON schema to follow (“Read the provided text and return a SINGLE JSON object with concise fields:”), but no demonstration of “steps” versus “constraint”.
By contrast, few-shot prompting augments the instructions by adding a small yet highly beneficial detail: it includes a small number of input-output examples within the prompt, teaching the model how to behave on the fly. Brown et al. (2020) demonstrated that in-context learning enables LLMs to generalise to new tasks with only a handful of examples, without requiring any parameter updates.
A third option is to allow the model more room to reason and think about its extractions. CoT, or Chain-of-Though prompts, explicitly ask the model to produce intermediate reasoning steps before giving any final answer.
Wei et al. (2022) and Kojima et al. (2022) demonstrate that generating such step-by-step rationales can substantially improve performance on multi-step reasoning tasks, both in few-shot and in zero-shot settings. While these models are mostly reported for very large models, the underlying core is appealing for our use case. Before emitting a structured JSON, the model can explicitly reason about which sentences are “actions” and which ones are “rules that modify actions”, rather than placing everything under steps.
Finally, recent research on structured outputs and schema-constrained decoding examines methods to make LLMs consistently generate machine-readable data that adheres to a user-provided schema, sometimes even optimising the schema itself for extraction quality. Our baseline already implements a simple schema in the prompt and performs validation after the fact. However, as the fire example shows, the semantics of constraints are still under-specified: ”requirements and prerequisites” is too vague, and nothing in the prompt prevents the model from simply turning every modal (“must”, “should”) into just another procedural step.
Building on these observations, we develop a series of prompting strategies that gradually enhance the guidance provided to the model:
	P0: Zero-shot Guided JSON Instruction Prompting (Baseline)
Our baseline regime, as described above, involves a single-shot, schema-guided instruction prompt with role assignment, explicit JSON fields (steps, constraints, entities), ID rules, and guidance on conciseness, although without any examples or explicit reasoning, and is used independently on each chunk.
	P1: Few-shot In-Context Learning (FS-ICL)
P1 enhances P0 with one or two annotated examples. Each example includes a short document fragment and its gold JSON extraction, where some obligations and guard conditions are intentionally encoded as constraints rather than steps. The rest of the schema and guidance remain the same. This approach tests whether in-context demonstrations help the model understand the difference between “do X” and “X must hold while/before doing Y,” thereby improving constraint coverage and attachment.
	P2: CoT-agumented JSON Prompting (CoT)
In P2, the model is first asked to reason in natural language about the procedural structure (“list the likely actions, then identify which sentences state conditions, rules, or safety requirements and which steps they modify”), and only then to produce the final JSON object. During decoding, we discard the free-text rationale and only parse the JSON block. This approach aims to encourage the model to explicitly classify sentences as actions versus constraints before committing to a structured representation, potentially enhancing ordering metrics and constraint identification.
	P3: Two-stage, schema-aware prompting
Finally, P3 breaks down the extraction into two LLM calls. In Stage 1, the model extracts only steps with stable IDs and local successor relations. In Stage 2, it receives the list of steps (IDs and texts) and is asked to produce only constraints and entities, with a clear rule that every constraint must reference at least one existing step ID. Outputs are validated against a JSON schema, and invalid references are rejected or corrected. This process enables tighter control over constraint attachment and prepares data for downstream PKG construction.
In the experimental section 5.4, we maintain the chunking strategy at our best semantic chunker (EBBC or DSC) and compare P0–P3 across both Tier A metrics (StepF1, AdjacencyF1, Constraint Coverage, and AttachmentF1) and Tier B graph metrics (GraphF1, NEXT EdgeF1, and LOGIC EdgeF1). We expect the few-shot and two-stage regimes (P₁ and P₃) to mainly increase constraint coverage and attachment quality, while CoT-style prompting (P₂) might produce more globally consistent step ordering and dependency structure. 
P1
…
P2
…
P3
…
Graph assembly (entity canonicalization, ID’ing, edge construction)
https://neo4j.com/blog/knowledge-graph/how-to-build-knowledge-graph/
Constraint validation & post-hoc repair (validators)
… 
Optional modules: retrieval-augmented context, human-in-the-loop review
RAG ? maybe
Human-in-the-loop = 100%
Experimental Setup & Results
Experimental design (systems compared, ablations)
Here we show how we will go from: baseline → +better chunking → +schema → +graph → +RAG - human-in-the-loop
Metrics (textual, order, graph, human; definitions you actually compute)
There are numerous ways to evaluate the quality of an IE system, they range from simple approaches like textual similarity where the system calculate text-level metrics such as ROUGE (short for Recall-Oriented Understudy for Gisting Evaluation) that compares the extracted text, to reference descriptions, or graph-level measures (e.g., node, edge, or order F1)) that assess how accurately the system reconstructs entities and relations, and finally human studies where the focus shifts to the perceived usefulness, correctness, and trustworthiness of the extracted knowledge. 
In their work, Rula & D’Souza 2023 tested two setups, “Zero-shot” where the model is asked to extract without examples or training, and “Few-shot” where the model is given a few examples or definitions before extraction, then they evaluated the models results by comparing them to a gold standard, a manually created “correct answer” representing an example of their desired perfect extraction results in two forms, plain text format (where the extracted information was sentences) and an ontologized format (well structured, where they for example would label each step with categories such as action, condition, object, etc.). The outcomes were that Few-shot learning performed much better and improved the quality of the extracted protocol, until the model had to fit its output into a strict ontology structure, where the accuracy dropped.
Recent works such as PAGED (Du et al. 2024) and Structured Information Extraction (Dagdelen et al. 2024) emphasise a two-level evaluation: (1) local step and order fidelity, and (2) graph coherence and constraint linking. Following this rationale, our evaluation separates flat textual extraction (Tier A) from structured procedural graph generation (Tier B), ensuring both tiers share identical gold data and alignment heuristics for comparability.
On a broad view, our goal is to assess whether LLMs can extract procedural knowledge that agents and digital twins can utilise, rather than merely providing flat lists of terms. To achieve that, the system compares (A) a flat baseline that generates unordered step candidates with (B) a structured system that produces a procedural knowledge graph (PKG). Both tiers share the same gold data and soft semantic matching; the structured tier is additionally evaluated with a graph metric and edge-type diagnostics, in line with current procedural KG practice and emerging benchmarks (Celino et al. 2024; Carriero et al. 2024; Du et al. 2024).
Common setup (for both tiers):
We align predicted and gold steps using sentence-level SBERT embeddings and a Hungarian 1-to-1 assignment (Kuhn 1955), adopting the soft semantic matching protocol used in PAGED (Du et al. 2024) and REBEL (Cabot & Navigli 2021). Matches are accepted above a similarity threshold empirically tuned on a development subset (τ≈0.75). This allows minor paraphrasing and lexical variation while preserving semantic equivalence.
Tier A: Flat (list) extraction
Evaluation performed on the output unordered list of candidate steps (optionally with condition phrases).
Metrics:
	StepF1 (primary): Precision/Recall/F1 over aligned step pairs (SBERT + Hungarian).
	AdjacencyF1 (order): Precision/Recall/F1 over NEXT bigrams formed from the matched sequence.
	Kendall (order, secondary): τ_b on matched step indices, reported in[0,1].
	Constraint metrics: measured at two levels:
	Constraint coverage: the fraction of gold constraint or guard phrases appearing anywhere in the output.
	Constraint attachment F1: the correctness of constraint-to-step binding (only for Tier B). 
This separation follows recommendations from Carriero et al. (2024) and Xu et al. (2024) for distinguishing semantic detection from structural linkage accuracy.
Tier-A headline score : 
A_score=0.7*Step_F1+0.3*Adjacency_F1
Tier B: Structured (graph) extraction.
Evaluation performed on the output PKG with nodes (steps and slot fields such as action/object/condition/parameter) and typed edges (e.g., PRECEDES/NEXT, GUARDS, REQUIRES).
Metrics:
	GraphF1 (primary): Smatch-style F1 over graph triples after best alignment, with light canonicalization (lowercasing, lemmatisation, synonym mapping) to avoid penalising paraphrases (Cai & Knight 2013).
	NEXT EdgeF1 (diagnostic): F1 over sequential edges only.
	Logic EdgeF1 (diagnostic): F1 over non-sequential edges (e.g., GUARDS, REQUIRES).
	Constraint AttachmentF1: F1 requiring correct (constraint text <=> type <=> attached step) triplets.
AdjacencyF1 and Kendall are also computed from the graph’s explicit order for comparability with Tier A.
Tier-B headline score: 
B_score=Graph_F1
Finally, to ensure that automatic metrics accurately reflect genuine semantic correctness, a subset (≈approximately 5-10%?) of instances is manually reviewed by us, following the human evaluation protocol proposed in Carriero et al. (2024). We will verify (i) completeness of the step list, (ii) faithfulness of ordering, and (iii) clarity of guard and constraint representation. This human sanity check complements the automatic metrics and mitigates alignment bias. 
Implementation details (models, params, hardware)
CONFIG DETAILS => copy .env
Results
Baseline performance with fixed-size chunking and zero-shot prompting
Table X summarises the performance of our initial system configuration, using fixed 2,000-character chunks and the P₀ zero-shot JSON prompting regime, evaluated on the three industrial procedures.
Table 4. Evaluation metric results of tests ran on baseline chunking/prompting configured system
Document	Step F1	Adjacency F1	Kendall τ	Constraint Coverage	Constraint Attachment F1	A-score
3M OEM SOP	0.620	0.267	0.736	0.000	0.000	0.514
DOA Food Man Proc Storage	0.168	0.000	0.750	0.625	0.000	0.118
Operational fire safety guideline	0.438	0.476	0.945	0.400	0.000	0.449
Macro average	0.409	0.248	0.810	0.342	0.000	0.360
Furthermore, we visualised the aggregated results right after
Ablations & error analysis (where adjacency & attachment fail)

Reproducibility (one-command pipeline; seeds; data splits)
Discussion 
Broader interpretation, limitations and future work ?
What improved & why (semantic chunking vs attachment, etc.)

Threats to validity (annotation noise, doc variety, model drift)

Limitations & future work

Conclusion